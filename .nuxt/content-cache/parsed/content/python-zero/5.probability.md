---
title: "5 Probability and Statistics"
description: "Using probability and statistics to analyze data"
navigation:
  index: 11
  icon: i-ph-info-duotone
---

## 1. What is Probability?
We all know what probability is - it's a way to describe the likelihood of something happening. For example, if I bet that today's cafeteria will serve Kung Pao Chicken, whether or not there will be Kung Pao Chicken becomes a probability event. Probability is closely related to gambling. For instance, during US election years, everyone tries to predict who will be the next president and what's the probability of Trump taking office. These possibilities can be estimated through calculations, which is why we study probability and statistics.

Next, we'll reference [《动手学习深度学习》--李沐](https://zh.d2l.ai/chapter_preliminaries/probability.html) to introduce probability theory. We'll also apply `Numpy` from Chapter 3 and use code and mathematical formulas for calculations.

## 2. Basic Probability Theory

Probability theory is a discipline that uses mathematical language to describe random events (a college course). It mainly studies the following basic concepts:

### Basic Definitions

- Random Events: Events with uncertainty in life

- Probability: A value between 0-1 that describes the likelihood of an event occurring
    - 0 means impossible to occur
    - 1 means certain to occur


Let's understand the basic concepts of probability theory through a simple lottery game.

- Basic probability calculation methods
- How to conduct probability experiments using Python
- How to understand probability distributions through visualization
- The relationship between theoretical and experimental probability

First, let's install and import the necessary packages.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}
```python
  import pyodide_js
  await pyodide_js.loadPackage("micropip")

  import micropip
  await micropip.install("numpy")
  await micropip.install("matplotlib")
```
::

#### Lottery Game Example

Suppose we have a lottery box containing:
- 30 red balls (first prize)
- 50 blue balls (second prize)
- 120 white balls (no prize)

Question: What is the probability of drawing each type of ball?

##### Theoretical Calculation

According to the basic definition of probability:
````
Probability of an event = Number of favorable outcomes ÷ Total number of possible outcomes
````

Therefore:
- Probability of drawing a red ball = 30/(30+50+120) = 0.15 (15%)
- Probability of drawing a blue ball = 50/(30+50+120) = 0.25 (25%)
- Probability of drawing a white ball = 120/(30+50+120) = 0.60 (60%)

#### Python Simulation Verification

Let's briefly explain that simulation verification is an important research method, and it's actually a specialized university course. We can use simulated data to infer the probability of an event. For example, when launching rockets, we can't actually launch a thousand times to determine the success rate. This is where we need to repeatedly simulate data to verify and continuously improve our success probability.

Now, let's use code to simulate this lottery process:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}
```python
import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Create prize pool (1=red, 2=blue, 3=white)
prize_pool = np.array([1]*30 + [2]*50 + [3]*120)

# Different numbers of trials
n_trials_list = [100, 1000, 10000]

print("Simulation Probability Results:")
print("-" * 40)
print("Trials\t\tRed\t\tBlue\t\tWhite")
print("Theory:\t\t0.150\t\t0.250\t\t0.600")
print("-" * 40)

# Perform simulations with different numbers of trials
for n_trials in n_trials_list:
    results = np.random.choice(prize_pool, size=n_trials)
    unique, counts = np.unique(results, return_counts=True)
    probabilities = counts / n_trials
    print(f"{n_trials}:\t\t{probabilities[0]:.3f}\t\t{probabilities[1]:.3f}\t\t{probabilities[2]:.3f}")
```
::

![Probability Distribution Simulation](../public/images/probability_simulation.png)


#### Key Points to Understand

1. **Sample Space**: The set of all possible outcomes, in this case, the collection of 200 balls

2. **Event**: A specific outcome from the sample space, such as drawing a red ball

3. **Probability**: 
   - Theoretical probability is calculated based on possibilities
   - Actual probability can be estimated through numerous experiments
   - As the number of experiments increases, the experimental probability approaches the theoretical probability

4. **Law of Large Numbers**: 
   - By increasing the number of trials (e.g., from 1,000 to 10,000), experimental results get closer to theoretical values
   - This is why we simulated large numbers of draws in our code

### 2.1 Probability Axioms

In our lottery game example, we've already introduced basic probability concepts. Now let's delve deeper into the fundamental axioms of probability theory.

Probability theory mainly studies two things:
1. Sample Space
2. Probability Function

#### Sample Space

The Sample Space is the set of all possible outcomes. In our lottery game:
- Sample Space S = {red ball, blue ball, white ball}
- Each ball represents a basic event
- Total of 200 balls constitutes the complete sample space

#### Probability Function

A Probability Function maps events to real numbers and must satisfy these axioms:

1. **Non-negativity**: For any event A, probability must be greater than or equal to 0
   - P(red ball) = 0.15 > 0
   - P(blue ball) = 0.25 > 0
   - P(white ball) = 0.60 > 0

2. **Normalization**: The sum of probabilities for all events in sample space S equals 1
   - P(red) + P(blue) + P(white) = 0.15 + 0.25 + 0.60 = 1

3. **Additivity**: For mutually exclusive events, the probability of their union equals the sum of their individual probabilities
   - Example: P(winning a prize) = P(red) + P(blue) = 0.15 + 0.25 = 0.40



#### Conditional Probability

In our lottery game, we can introduce the concept of conditional probability. For example:
- If we know a prize was won, what's the probability it was a red ball?
- P(red|win) = P(red) / P(win) = 0.15 / 0.40 = 0.375

This means that given we won a prize, there's a 37.5% chance it's the first prize (red ball).

#### Independence

Two events A and B are independent if the occurrence of one event doesn't affect the probability of the other. In our lottery game:
- With replacement: Each draw is independent
- Without replacement: Events are not independent, as previous results affect subsequent probabilities

### 2.2 Random Variables

A random variable is a core concept in probability theory, representing a function mapping from the sample space to real numbers. Simply put, it's a numerical representation of random experiment outcomes.

#### Discrete Random Variables

A discrete random variable can only take on a finite or countably infinite number of values.

Let's continue with our lottery example:
- X = Prize value
  - Red ball (first prize) = 1000 yuan
  - Blue ball (second prize) = 500 yuan
  - White ball (no prize) = 0 yuan

Here, X is a discrete random variable because it can only take these three specific values.


::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}
```python
# Second code block - Lottery Simulation
import numpy as np

# Define prize values
prizes = {
    1: 1000,  # Red ball worth 1000 yuan
    2: 500,   # Blue ball worth 500 yuan
    3: 0      # White ball worth 0 yuan
}

# Simulate 1000 draws
np.random.seed(42)
draws = np.random.choice([1, 2, 3], size=1000, p=[0.15, 0.25, 0.60])

# Calculate expected value (average prize money)
expected_value = sum(prizes[ball] * count/1000 
                    for ball, count in zip(*np.unique(draws, return_counts=True)))

print("Lottery Simulation Results:")
print(f"Average prize: {expected_value:.2f} yuan")
print("\nFrequency of each outcome:")
for ball, count in zip(*np.unique(draws, return_counts=True)):
    print(f"Prize worth {prizes[ball]} yuan: {count} times ({count/10:.1f}%)")
```
::

#### Continuous Random Variables

A continuous random variable can take any real value within an interval.

For example: The lifespan X (in hours) of light bulbs produced by a factory
- X can be any real number between 1000 and 2000 hours
- The probability of different lifespans is described by a probability density function

Let's simulate the distribution of bulb lifespans:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}
```python
# First code block - Light Bulb Lifetime Simulation
import numpy as np

# Generate 1000 light bulb lifetimes following normal distribution
# Mean: 1500 hours, Standard deviation: 100 hours
np.random.seed(42)
lifetimes = np.random.normal(1500, 100, 1000)

# Calculate basic statistics
mean_lifetime = np.mean(lifetimes)
std_lifetime = np.std(lifetimes)
min_lifetime = np.min(lifetimes)
max_lifetime = np.max(lifetimes)

print("Light Bulb Lifetime Statistics:")
print(f"Average lifetime: {mean_lifetime:.2f} hours")
print(f"Standard deviation: {std_lifetime:.2f} hours")
print(f"Minimum lifetime: {min_lifetime:.2f} hours")
print(f"Maximum lifetime: {max_lifetime:.2f} hours")

# Calculate number of bulbs in different lifetime intervals
intervals = [1200, 1400, 1600, 1800]
counts = np.histogram(lifetimes, bins=intervals)[0]
print("\nLifetime Distribution:")
for i in range(len(counts)):
    start = intervals[i]
    end = intervals[i+1]
    count = counts[i]
    print(f"{start}-{end} hours: {count} bulbs ({count/10:.1f}%)")
```
::

#### Key Characteristics of Random Variables

1. **Expected Value (E(X))**
   - Discrete: Sum of each possible value multiplied by its probability
   - Continuous: Integral of the probability density function times the value

2. **Variance (Var(X))**
   - Describes the spread of the random variable
   - Larger variance indicates more dispersed data

3. **Distribution Functions**
   - Discrete: Probability Mass Function (PMF)
   - Continuous: Probability Density Function (PDF)

#### Practical Applications

Random variables have wide applications in real life:

1. **Quality Control**
   - Product dimensions (continuous)
   - Number of defects (discrete)

2. **Financial Analysis**
   - Stock prices (continuous)
   - Daily transaction count (discrete)

3. **Medical Research**
   - Patient recovery time (continuous)
   - Daily new cases (discrete)

Understanding the types and characteristics of random variables is crucial for proper modeling and analysis of real-world problems. In machine learning, we frequently deal with both types of random variables and must choose appropriate models and algorithms based on their characteristics.


## 3. Handling Multiple Random Variables

In the real world, we often need to deal with multiple random variables simultaneously. Let's understand this through a medical diagnosis example:

Suppose we are interested in two random variables:
- D: Whether a patient has a certain disease (positive or negative)
- T: The result of the diagnostic test (positive or negative)

This example involves several types of probabilities:

### 3.1 Basic Probability Types

1. **Joint Probability P(D,T)**
   - The probability of two events occurring simultaneously
   - Example: P(having disease AND testing positive)

2. **Marginal Probability P(D) or P(T)**
   - The probability of a single event occurring
   - Example: P(having disease) or P(testing positive)

3. **Conditional Probability P(D|T)**
   - The probability of one event occurring given that another event has occurred
   - Example: P(having disease|testing positive) = probability of actually having the disease when test is positive

### 3.2 Bayes' Theorem: The Cornerstone of Probability Theory

Bayes' Theorem is one of the most important theorems in probability theory, providing a mathematical framework for updating beliefs. In machine learning and artificial intelligence, it forms the theoretical foundation for many algorithms.

#### Mathematical Formula

````
$$P(D|T) = \frac{P(T|D)P(D)}{P(T)}$$
````

Where:
- P(D|T) is the posterior probability: new understanding of D after observing evidence T
- P(T|D) is the likelihood: probability of observing T when D is true
- P(D) is the prior probability: understanding of D before observing T
- P(T) is the normalization constant: sum of probabilities for all possible scenarios

#### Code Implementation of Medical Diagnosis Case

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}
```python
import numpy as np

# Known probabilities
P_disease = 0.01  # Disease prevalence (prior probability)
P_positive_given_disease = 0.95  # Probability of positive test if diseased (sensitivity)
P_negative_given_healthy = 0.90  # Probability of negative test if healthy (specificity)

# Calculate P(T): Total probability of positive test
P_positive = (P_positive_given_disease * P_disease + 
             (1 - P_negative_given_healthy) * (1 - P_disease))

# Calculate posterior probability using Bayes' theorem
P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive

print("Medical Diagnosis Probability Analysis:")
print(f"Disease prevalence: {P_disease:.1%}")
print(f"Test accuracy (sensitivity): {P_positive_given_disease:.1%}")
print(f"Test specificity: {P_negative_given_healthy:.1%}")
print(f"\nProbability of having disease if test is positive: {P_disease_given_positive:.1%}")
```
::

#### Importance of Bayes' Theorem

1. **Quantification of Uncertainty**
   - Helps update probability estimates when new information is received
   - Provides a mathematical framework for handling uncertainty

2. **Machine Learning Applications**
   - Naive Bayes classifiers
   - Bayesian neural networks
   - Probabilistic graphical models

3. **Decision Support**
   - Medical diagnosis
   - Risk assessment
   - Scientific reasoning

4. **Foundation of Modern AI**
   - Probabilistic reasoning
   - Uncertainty modeling
   - Adaptive learning systems

Bayes' theorem is not just a mathematical formula; it's a way of thinking. It teaches us how to rationally update our beliefs when new evidence emerges, which is particularly crucial in today's data-driven age. Whether in medical diagnosis, spam filtering, or artificial intelligence systems, Bayesian methods play a vital role.


## GPU and Deep Learning

### Rise and Impact of GPU

Graphics Processing Units (`GPUs`) were initially designed for graphics and gaming. However, researchers discovered that GPU's parallel computing architecture is highly suitable for handling large-scale mathematical computations in machine learning. Since 2000, GPU performance has improved approximately 1000-fold every decade, providing a powerful hardware foundation for deep learning development.

`NVIDIA` revolutionized the field by introducing `CUDA` (Compute Unified Device Architecture), transforming GPUs from mere graphics processors into general-purpose parallel computing processors. This transformation fundamentally changed the trajectory of deep learning. While traditional CPUs excel at serial computations, GPUs can handle thousands of parallel tasks simultaneously, perfectly matching the requirements of large-scale matrix operations in deep learning.

### Revolutionary Impact on Machine Learning

1. **Quantum Leap in Training Speed**: Model training that once took weeks can now be completed in hours, greatly accelerating algorithm development and model optimization.

2. **Enabling Larger Scale Models**: From early neural networks with few layers to today's models with hundreds of billions of parameters (like the GPT series), this development wouldn't be possible without GPU computing power.

3. **Lower Entry Barriers**: Researchers no longer need expensive supercomputers; a personal computer equipped with a GPU is sufficient for deep learning research.

### Applications in Probability and Statistics

In probability and statistics, GPU's parallel computing capability has brought revolutionary changes. For example, in Monte Carlo simulations, we can simultaneously conduct millions of random trials, which would be impractical on traditional CPUs. This enhanced computing power enables us to:
- Process larger datasets
- Make more precise probability estimates
- Implement more complex statistical models

### Real-world Case: Weather Forecasting Model

Let's illustrate the importance of GPUs through weather forecasting:

Traditional weather forecasting models must consider millions of data points (temperature, pressure, wind speed, etc.) and perform extensive probability calculations to predict future weather. In the CPU era, producing an accurate 48-hour weather forecast could take 24 hours of computation time, meaning the forecast results would be available too late to be optimally useful.

With GPU acceleration:
1. Computation time can be reduced to 1-2 hours
2. Multiple weather scenarios can be calculated simultaneously
3. Higher resolution meteorological data can be processed
4. Forecast accuracy and timeliness are significantly improved

This example effectively demonstrates how GPUs, through their powerful parallel computing capabilities, have transformed traditional computation-intensive tasks. Similar revolutionary changes are occurring in financial analysis, gene sequencing, drug development, and many other fields.

### Future Outlook

As AI technology continues to evolve, the demand for computing power keeps growing. The emergence of next-generation GPU architectures (like NVIDIA's Hopper) and specialized AI chips indicates we're entering an era of explosive growth in computing power. This will bring more possibilities to machine learning and deep learning, pushing AI technology toward broader and deeper development.

[Reference: Deep Learning Computation - Using GPU](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)