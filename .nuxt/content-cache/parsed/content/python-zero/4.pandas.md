---
title: "4 Getting Started with pandas"
description: "Using pandas to manipulate data"
navigation:
  index: 9
  icon: i-ph-info-duotone
---

## 1. Introduction to Pandas

Pandas is an open-source data analysis and data processing library based on the Python programming language.

Pandas provides easy-to-use data structures and data analysis tools, especially suitable for handling structured data, such as tabular data (similar to Excel spreadsheets).

Pandas is one of the commonly used tools in the field of data science and analysis, enabling users to easily import data from various data sources and perform efficient operations and analysis on the data.

Pandas offers a wealth of features, including:

- Data cleaning: Handling missing data, duplicate data, etc.
- Data transformation: Changing the shape, structure, or format of data.
- Data analysis: Performing statistical analysis, aggregation, grouping, etc.
- Data visualization: Integrating with libraries like Matplotlib and Seaborn for data visualization.

## 2. Data Structures in Pandas

Pandas mainly introduces two new data structures: **DataFrame** and **Series**.

- **Series**: Similar to a one-dimensional array or list, it consists of a set of data and associated data labels (indexes). A Series can be considered as a column in a DataFrame or as a standalone one-dimensional data structure.
- **DataFrame**: Similar to a two-dimensional table, it is the most important data structure in Pandas. A DataFrame can be seen as a table composed of multiple Series arranged by columns, with both row and column indexes, making it convenient for row and column selection, filtering, merging, and other operations.

> [!Note]
> A DataFrame can be considered as a data structure composed of multiple Series.

### 1. Pandas Series

A Pandas `Series` is a flexible, one-dimensional, indexed data structure that can store various types of data (such as integers, floats, strings, timestamps, etc.). Each data item has an associated label (index), similar to key-value pairs in a dictionary. The main features of a `Series` include:

- **One-dimensional array structure**: A `Series` is a one-dimensional data structure, similar to a NumPy array but more flexible.
- **Indexing functionality**: Each data item has a unique index, which can be default integer values (like 0, 1, 2...) or custom (like characters, dates, etc.). Indexing allows for quick access and slicing.
- **Flexible data types**: A `Series` supports various data types, including integers, floats, booleans, strings, timestamps, etc.
- **Automatic alignment operations**: When operating on multiple `Series` objects, Pandas automatically aligns data based on the index.
- **Handling missing values**: A `Series` supports handling missing values (usually represented by `NaN`) and can intelligently ignore missing values during calculations.
- **Immutable size**: Once a `Series` is created, its size is fixed, but new objects can be generated to adjust the size using methods like `append` and `drop`.

We can create a `Series` object using the `pd.Series()` constructor, passing in data (such as lists, NumPy arrays, etc.) and an optional index array.

```python
# Constructor and parameters
pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)
```

Parameter descriptions:

- **`data`**: The data part of the `Series`, which can be a list, array, dictionary, scalar value, etc. If this parameter is not provided, an empty `Series` will be created.
- **`index`**: The index part of the `Series`, used to label each data item. It can be a list, array, or index object, etc. If not specified, a default integer index starting from 0 will be generated.
- **`dtype`**: Specifies the data type of the `Series`. Supports NumPy data types, such as `np.int64`, `np.float64`, etc. If not specified, the system will automatically infer the type based on the input data.
- **`name`**: Specifies a name for the `Series` object. If this parameter is provided, the generated `Series` will have the corresponding name.
- **`copy`**: Whether to copy the data. If `True`, the input data will be copied. The default value is `False`, meaning no copy.
- **`fastpath`**: Whether to enable the fast path. The default is `False`. This parameter is generally used for internal optimization and may improve performance when enabled.

![](http://szms-python-images.oss-cn-hangzhou.aliyuncs.com/Pandas/%E6%96%B0%E7%89%88%E8%84%91%E5%9B%BE/%E5%88%9B%E5%BB%BAseries%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95en.png)

Here is a basic example showing how to create a `Series` using a list:

```python
import pandas as pd

# Create a Series
data = pd.Series([100, 200, 300, 400])

# Output the Series
print(data)
```

Output:

```shell
0    100
1    200
2    300
3    400
dtype: int64
```

In this example, the `Series` automatically uses integer indexes starting from 0. We can access specific data using the index value:

```python
print(data[1])  # Output 200
```

Additionally, we can create a `Series` with custom indexes:

```python
import pandas as pd

# Create a Series with custom indexes
data = pd.Series([100, 200, 300, 400], index=["A", "B", "C", "D"])

# Output the Series
print(data)
```

Output:

```shell
A    100
B    200
C    300
D    400
dtype: int64
```

At this point, the indexes are `A`, `B`, `C`, and `D`. We can use the index value to access the corresponding data:

```python
print(data["A"])  # Output 100
```

A `Series` can also be created by passing a dictionary. The keys of the dictionary will be used as the indexes of the `Series`, and the values will be used as the data of the `Series`:

```python
import pandas as pd

# Create a Series using a dictionary
data_dict = {"A": 10, "B": 20, "C": 30, "D": 40}
data_series = pd.Series(data_dict)

# Output the Series
print(data_series)
```

Output:

```shell
A    10
B    20
C    30
D    40
dtype: int64
```

If not all data from the dictionary is needed, we can select part of the dictionary data by specifying an index when creating the `Series`:

```python
# Select only part of the indexes
partial_series = pd.Series(data_dict, index=["A", "C"])
print(partial_series)
```

Output:

```shell
A    10
C    30
dtype: int64
```

In summary, a Pandas `Series` is a very flexible and powerful data structure suitable for handling one-dimensional data with indexes. Whether it is a simple data list or complex data constructed through dictionaries and arrays, a `Series` provides powerful operations and retrieval functions, especially suitable for scenarios involving labeled data.

### 2. Pandas DataFrame

A Pandas `DataFrame` is a two-dimensional, labeled data structure similar to a spreadsheet or SQL table. It can store various types of data (such as integers, floats, strings, timestamps, etc.) and can combine different types of data for processing. The `DataFrame` is one of the most commonly used data structures in the Pandas library, widely used for data analysis, cleaning, and manipulation.

The main features of a `DataFrame` include:

- **Two-dimensional data structure**: A `DataFrame` is a tabular data structure with rows and columns, supporting multiple data types.
- **Flexible row and column indexes**: Both rows and columns have corresponding labels (indexes), supporting custom or automatically generated integer indexes. Indexes can be used for quick data location.
- **Heterogeneous data types**: Different columns can store different types of data (e.g., one column can be strings, another can be floats).
- **Automatic alignment operations**: When operating on multiple `DataFrame` objects, Pandas automatically aligns data based on row and column indexes.
- **Handling missing values**: A `DataFrame` supports missing values, usually represented by `NaN`, and provides rich methods to handle missing data.
- **Rich operation functions**: A `DataFrame` provides flexible operations on data, such as adding and deleting rows and columns, filtering, sorting, merging, and grouping.

We can create a `DataFrame` using the `pd.DataFrame()` constructor, passing in data in various forms (such as dictionaries, lists, NumPy arrays, etc.).

```python
# Constructor and parameters
pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)
```

Parameter descriptions:

- **`data`**: The data part of the `DataFrame`, which can be a dictionary, list, two-dimensional array, NumPy array, or another `DataFrame`. Different data structures have different default behaviors. For example, dictionary keys are used as column names, and lists are used as row data.
- **`index`**: The row index of the `DataFrame`. If not specified, Pandas will generate an integer index starting from 0.
- **`columns`**: The column labels of the `DataFrame`. If not specified, Pandas will automatically use the keys of the input data (e.g., dictionary keys) or integer indexes starting from 0.
- **`dtype`**: Specifies the data type of the `DataFrame`. It can be a NumPy data type. If not specified, Pandas will automatically infer the type based on the input data.
- **`copy`**: Whether to copy the data. The default is `False`, meaning no copy. If set to `True`, the input data will be copied.

Here are some simple examples of creating a `DataFrame`

![](http://szms-python-images.oss-cn-hangzhou.aliyuncs.com/Pandas/%E6%96%B0%E7%89%88%E8%84%91%E5%9B%BE/%E5%88%9B%E5%BB%BAdataframe%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95en.png)

One of the most common ways to create a `DataFrame` is by using a dictionary, where the keys of the dictionary become the column names and the values become the column data:

```python
import pandas as pd

# Create DataFrame using a dictionary
data = {
    "Name": ["Tom", "Jerry", "Spike"],
    "Age": [28, 22, 25],
    "Score": [85, 92, 78]
}

df = pd.DataFrame(data)

# Output DataFrame
print(df)
```

Output:

```shell
    Name  Age  Score
0    Tom   28     85
1  Jerry   22     92
2  Spike   25     78
```

In this example, the keys `Name`, `Age`, and `Score` of the dictionary become the column names, and the list data becomes the values of each column. Pandas automatically generates integer indexes starting from 0.

Besides using a dictionary, we can also create a `DataFrame` using a list of nested lists, where each nested list represents a row of data:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create DataFrame using nested lists
data = [
    ["Tom", 28, 85],
    ["Jerry", 22, 92],
    ["Spike", 25, 78]
]

df = pd.DataFrame(data, columns=["Name", "Age", "Score"])

# Output DataFrame
print(df)
```

::

Here, the `columns` parameter explicitly specifies the column names, and each sublist in the list corresponds to a row in the `DataFrame`.

Based on the flexibility of pandas, we can also create a DataFrame using a NumPy array, which is suitable for large numerical datasets:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd
import numpy as np

# Create DataFrame using a NumPy array
data = np.array([[85, 28], [92, 22], [78, 25]])

df = pd.DataFrame(data, columns=["Score", "Age"], index=["Tom", "Jerry", "Spike"])

# Output DataFrame
print(df)
```

::

In this example, we use the `index` parameter to specify custom labels for the rows of the `DataFrame`.

Finally, if the data is stored in the form of a list of dictionaries (such as JSON), we can directly pass the list of dictionaries:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create DataFrame using a list of dictionaries
data = [
    {"Name": "Tom", "Age": 28, "Score": 85},
    {"Name": "Jerry", "Age": 22, "Score": 92},
    {"Name": "Spike", "Age": 25, "Score": 78}
]

df = pd.DataFrame(data)

# Output DataFrame
print(df)
```

::

For `DataFrame`, the following common operations are available:

1. Accessing columns: You can access specific columns by column name.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create DataFrame
data = {
    "Name": ["Tom", "Jerry", "Spike"],
    "Age": [28, 22, 25],
    "Score": [85, 92, 78]
}
df = pd.DataFrame(data)

# Access the Name column
print(df["Name"])
```

::

2. Adding new columns: You can easily add new columns using assignment statements.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Add a new column City
df["City"] = ["NY", "LA", "SF"]

# Output DataFrame
print(df)
```

::

3. Deleting columns: Use the `drop()` method to delete columns.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Delete the Age column
df_dropped = df.drop("Age", axis=1)

# Output the DataFrame after deletion
print(df_dropped)
```

::

4. Filtering data: You can filter data using conditional expressions.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Filter data with Score greater than 80
filtered_df = df[df["Score"] > 80]

# Output the filtered results
print(filtered_df)
```

::

5. Calculating statistical information: `DataFrame` provides rich statistical methods. The following example shows how to calculate the mean and descriptive statistics:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Calculate the mean
mean_score = df["Score"].mean()
print(f"Average Score: {mean_score}")

# Output descriptive statistics of the DataFrame
print(df.describe())
```

::

6. Handling missing values: You can check for missing values using `isna()` and fill missing values using `fillna()`. The following example shows how to handle missing data:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Create a DataFrame with missing values
data_with_na = {
    "Name": ["Tom", "Jerry", "Spike"],
    "Age": [28, None, 25],
    "Score": [85, 92, None]
}
df_with_na = pd.DataFrame(data_with_na)

# Check for missing values
print(df_with_na.isna())

# Fill missing values
df_filled = df_with_na.fillna(0)

# Output the DataFrame after filling missing values
print(df_filled)
```

::

In summary, the Pandas `DataFrame` is a powerful and flexible two-dimensional data structure widely used in various data analysis tasks. Whether handling numerical data, time series, or structured tabular data, the `DataFrame` provides efficient and convenient methods for operations and processing.

### 3. Case Study

Solidify the data structures and basic operations in Pandas through the case exercises in the link below.

[https://www.kaggle.com/code/zhaowenxi/pandas-for-python-1](https://www.kaggle.com/code/zhaowenxi/pandas-for-python-1)

## 3. Pandas and Common File Formats

### 1. Pandas and CSV

CSV (Comma-Separated Values) is a common file format known for its simplicity and lightweight nature, widely used for data transfer and storage. Pandas is a powerful tool in Python for data analysis and processing, which can easily read, analyze, and process CSV files. This article uses a classic `example.csv` file as an example to demonstrate common Pandas operations. Save the following data locally and store it in .csv format, and perform operations with the code.

```csv
ID,Name,Age,Gender,Department,Salary,HireDate,PerformanceScore,City
1,John Smith,28,Male,Sales,50000,2018-05-21,85,New York
2,Mary Johnson,34,Female,HR,62000,2017-03-15,92,Los Angeles
3,James Brown,45,Male,IT,72000,2015-09-30,78,Chicago
4,Patricia Garcia,29,Female,Finance,55000,2019-07-11,88,Houston
5,Robert Martinez,41,Male,Marketing,68000,2016-04-20,80,Miami
6,Linda Anderson,36,Female,Sales,59000,2018-11-12,90,Boston
7,Michael Wilson,38,Male,HR,63000,2017-06-07,86,San Francisco
8,Elizabeth Taylor,30,Female,IT,74000,2019-02-23,75,Dallas
9,David Lee,25,Male,Finance,52000,2020-08-14,82,Austin
10,Barbara Hernandez,32,Female,Marketing,67000,2017-10-19,83,Seattle
11,William Clark,29,Male,Sales,56000,2019-12-01,81,Denver
12,Jennifer Lewis,35,Female,HR,60000,2018-01-27,87,Las Vegas
13,Joseph Walker,40,Male,IT,73000,2015-05-10,76,San Diego
14,Susan Hall,33,Female,Finance,56000,2018-09-18,89,New Orleans
15,Charles Young,50,Male,Marketing,78000,2014-11-05,90,Portland
16,Karen King,26,Female,Sales,48000,2020-02-10,84,Columbus
17,Thomas Wright,37,Male,HR,61000,2016-07-15,82,Charlotte
18,Lisa Scott,31,Female,IT,75000,2019-04-29,74,Detroit
19,Christopher Green,47,Male,Finance,70000,2014-06-19,79,Indianapolis
20,Amy Adams,27,Female,Marketing,53000,2019-01-20,85,Nashville
21,Mark Baker,44,Male,Sales,68000,2015-10-03,88,Jacksonville
22,Sarah Gonzalez,39,Female,HR,62000,2017-08-23,81,Memphis
23,Paul Nelson,33,Male,IT,72000,2016-05-09,77,Fort Worth
24,Donna Mitchell,42,Female,Finance,69000,2016-12-27,91,El Paso
25,Steven Perez,28,Male,Marketing,55000,2020-03-05,80,Milwaukee
26,Nancy Roberts,31,Female,Sales,52000,2019-10-15,86,Baltimore
27,Matthew Turner,46,Male,HR,60000,2014-08-21,82,Louisville
28,Dorothy Phillips,34,Female,IT,76000,2016-01-12,74,Oklahoma City
29,Ronald Campbell,43,Male,Finance,72000,2015-03-19,87,Albuquerque
30,Laura Parker,30,Female,Marketing,57000,2020-06-07,80,Kansas City
```

![](http://szms-python-images.oss-cn-hangzhou.aliyuncs.com/Pandas/%E6%96%B0%E7%89%88%E8%84%91%E5%9B%BE/%E4%B8%80%E8%88%AC%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8Ben.png)

1. Read CSV file: Use `pd.read_csv()` to read a CSV file into a Pandas `DataFrame` object.

```python
import pandas as pd

# Read CSV file
df = pd.read_csv('example.csv')

# Display the first few rows of data
print(df.to_string())
```

> [!Note]
> `to_string()` is used to print the entire DataFrame, avoiding Pandas' default behavior of displaying only the first and last 5 rows. If the file is large, it is not recommended to use this method as the output can be very large.

2. View specified head and tail rows:

    - Use `head(n)` to display the first `n` rows of the dataset. If `n` is not specified, the first 5 rows are returned by default.
    - Use `tail(n)` to display the last `n` rows of the dataset. If `n` is not specified, the last 5 rows are returned by default.

```python
# Display the first 5 rows (default)
print(df.head())

# Display the first 10 rows
print(df.head(10))

# Display the last 5 rows (default)
print(df.tail())

# Display the last 10 rows
print(df.tail(10))

```

3. Get basic information: Use the `info()` method to get basic information about the data, such as column names, data types, and the number of non-null values.

```python
# Get basic information about the data
df.info()
```

4. Calculate statistics: The `describe()` method can quickly get statistical information for numerical columns, such as mean, standard deviation, minimum, and maximum values.

```python
# Calculate statistical information for numerical columns
print(df.describe())
```

5. Filter data: You can filter data based on conditions, such as filtering employees older than 30 and with a salary over 60000.

```python
filtered_df = df[(df["Age"] > 30) & (df["Salary"] > 60000)]
print(filtered_df)
```

6. Save the modified DataFrame

```python
# Save as a new CSV file
df.to_csv('example_modified.csv', index=False)
```

### 2. Pandas and JSON

Pandas is also very efficient at handling JSON data, converting JSON data structures into DataFrames, thereby simplifying complex nested data formats. JSON (JavaScript Object Notation) is a lightweight data interchange format widely used in web development and API responses. This article will demonstrate how to use Pandas to handle JSON data.

We use a JSON data example containing employee information:

```json
[
    {"ID": 1, "Name": "John Smith", "Age": 28, "Gender": "Male", "Department": "Sales", "Salary": 50000, "HireDate": "2018-05-21", "PerformanceScore": 85, "City": "New York"},
    {"ID": 2, "Name": "Mary Johnson", "Age": 34, "Gender": "Female", "Department": "HR", "Salary": 62000, "HireDate": "2017-03-15", "PerformanceScore": 92, "City": "Los Angeles"},
    {"ID": 3, "Name": "James Brown", "Age": 45, "Gender": "Male", "Department": "IT", "Salary": 72000, "HireDate": "2015-09-30", "PerformanceScore": 78, "City": "Chicago"}
]
```

1. Read JSON file: Pandas can directly read JSON files using `pd.read_json()`. The JSON data format supports lists, dictionaries, and more complex nested structures.

```python
import pandas as pd

# Read JSON file
df = pd.read_json('example.json')

# Display data
print(df.to_string())
```

2. Handle JSON with nested structures: JSON files may contain nested structures, and Pandas can handle these nested fields. For deeply nested JSON structures, it is usually necessary to first flatten or extract the nested fields into separate columns.

Assume the JSON data has a nested `Address` field:

```json
{
    "ID": 1,
    "Name": "John Smith",
    "Age": 28,
    "Address": {"City": "New York", "PostalCode": "10001"},
    "Department": "Sales",
    "Salary": 50000,
    "HireDate": "2018-05-21"
}
```

You can use Pandas' `json_normalize` method to flatten the nested fields:

```python
import pandas as pd
from pandas import json_normalize

# Nested JSON data
data = {
    "ID": 1,
    "Name": "John Smith",
    "Age": 28,
    "Address": {"City": "New York", "PostalCode": "10001"},
    "Department": "Sales",
    "Salary": 50000,
    "HireDate": "2018-05-21"
}

# Use json_normalize to flatten nested fields
df = json_normalize(data)
print(df.to_string())
```

Output:

```shell
   ID        Name  Age  Department  Salary    HireDate  Address.City  Address.PostalCode
0   1  John Smith   28      Sales    50000  2018-05-21     New York              10001
```

For complex JSON files, such as multi-layer nested JSON data from APIs, `json_normalize` can also flatten multiple nested layers based on the path. For example, assume the JSON data has more nested levels:

```json
{
    "ID": 1,
    "Name": "John Smith",
    "Details": {
        "Age": 28,
        "Address": {
            "City": "New York",
            "PostalCode": "10001"
        }
    },
    "Department": "Sales",
    "Salary": 50000,
    "HireDate": "2018-05-21"
}
```

You can specify the path to flatten multiple nested layers:

```python
import pandas as pd
from pandas import json_normalize

# Complex nested JSON data
data = {
    "ID": 1,
    "Name": "John Smith",
    "Details": {
        "Age": 28,
        "Address": {
            "City": "New York",
            "PostalCode": "10001"
        }
    },
    "Department": "Sales",
    "Salary": 50000,
    "HireDate": "2018-05-21"
}

# Flatten nested layers
df = json_normalize(data, sep='_')
print(df.to_string())
```

Output:

```shell
   ID        Name  Details_Age  Details_Address_City  Details_Address_PostalCode  Department  Salary    HireDate
0   1  John Smith           28              New York                      10001       Sales   50000  2018-05-21
```

Here we used `sep='_'` to control the naming format of the flattened nested fields, making the DataFrame clearer and easier to read.

3. Save DataFrame as JSON: After processing the JSON data, you can save it back to JSON format. Pandas provides the `to_json()` method to accomplish this task.

```python
# Save DataFrame as JSON file
df.to_json('output.json', orient='records', lines=True)
```

- `orient='records'` specifies to output each row as a JSON object.
- `lines=True` is used to output a multi-line JSON file, with each line being a separate JSON record, suitable for handling large datasets.

In summary, Pandas is a powerful tool for handling JSON data, especially for complex nested JSON. Pandas provides convenient tools to convert data into tabular format for easy analysis and manipulation. With Pandas, we can easily read, process, and store JSON data, simplifying the data analysis process.

## 4. Pandas and Data Cleaning

Data cleaning is the process of handling useless data. In practice, it is common to encounter missing data, incorrect data formats, erroneous data, or duplicate data. To make the results of data analysis more accurate, these useless data need to be processed. Pandas provides a variety of functions to help clean, inspect, and correct issues in the data.

### 1. Handling Missing Values

Handling missing values is a very common and fundamental task in data cleaning. Pandas provides rich methods to find, count, and handle these missing values. Below, we will explain in detail how to effectively use Pandas to handle missing values, especially the best practices for deleting missing values.

![](http://szms-python-images.oss-cn-hangzhou.aliyuncs.com/Pandas/%E6%96%B0%E7%89%88%E8%84%91%E5%9B%BE/%E7%A9%BA%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86en.png)

1. Find Missing Values: To find missing values in the data, you can use the `isna()` or `isnull()` methods. These two methods are equivalent and will return a boolean DataFrame where `True` indicates the value is missing.

```python
import pandas as pd

# Create data with missing values
data = {
    "Name": ["Alice", "Bob", None],
    "Age": [25, None, 30],
    "Salary": [50000, 60000, None]
}
df = pd.DataFrame(data)

# Find missing values
print(df.isna())
```

Output:

```shell
    Name    Age  Salary
0  False  False   False
1  False   True   False
2   True  False    True
```

> [!Note]
> It should be noted that in some cases, missing values may not only appear as standard `NaN`, but also in other forms such as `"n/a"` or `"--"`. Pandas provides the `na_values` parameter, allowing us to specify these custom missing value types when reading data.
> ```python
> # Custom missing value forms
> missing_values = ["n/a", "na", "--"]
> df = pd.read_csv('example.csv', na_values=missing_values)
> ```

2. Count Missing Values: You can combine the `isna()` and `sum()` methods to count the number of missing values in each column. The boolean values returned by `isna()` can be summed using `sum()`, showing the number of missing values in each column.

```python
# Count the number of missing values in each column
print(df.isna().sum())
```

Output:

```shell
Name      1
Age       1
Salary    1
dtype: int64
```

3. Delete Missing Values: When handling missing values, deleting rows or columns containing missing values is a common operation, especially when the proportion of missing data is large. You can use the `dropna()` method to delete missing values. `dropna()` provides a variety of flexible parameters to decide whether to delete entire rows or columns, or only under certain conditions.

```python
# Syntax:
DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)
```

Parameter explanation:

- `axis`: Specifies the direction of deletion. The default is `0`, which deletes entire rows containing missing values; if set to `1`, it deletes entire columns containing missing values.
- `how`: Specifies the condition for deletion. The default value is `'any'`, which deletes if any value in a row or column is missing; if set to `'all'`, it deletes only if all values in a row or column are missing.
- `thresh`: Sets a threshold, requiring the row or column to have at least `thresh` non-missing values to be retained. For example, if `thresh=2`, it means retaining rows or columns with at least two non-missing values.
- `subset`: Specifies the columns to check for missing values. If you only need to check specific columns, you can pass a list of column names.
- `inplace`: If set to `True`, the deletion operation will be performed on the original DataFrame and will not return a new object; if `False` (default), it will return a new DataFrame.

Run the following examples separately to see the comparison with the original `DataFrame`:

```python
# Delete rows containing any missing values
df_cleaned = df.dropna()
print(df_cleaned)

# Delete rows with missing values only in 'Name' and 'Age' columns
df_cleaned_subset = df.dropna(subset=['Name', 'Age'])
print(df_cleaned_subset)

# Delete rows containing all missing values
df_cleaned_all = df.dropna(how='all')
print(df_cleaned_all)

# Retain rows with at least two non-missing values
df_thresh = df.dropna(thresh=2)
print(df_thresh)
```

Output:

```shell
# Original DataFrame
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob   NaN  60000.0
2   None  30.0      NaN

# Delete rows containing any missing values:
    Name   Age   Salary
0  Alice  25.0  50000.0

# Delete rows with missing values only in 'Name' and 'Age' columns:
    Name   Age   Salary
0  Alice  25.0  50000.0

# Delete rows containing all missing values:
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob   NaN  60000.0
2   None  30.0      NaN

# Retain rows with at least two non-missing values:
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob   NaN  60000.0
```

4. Fill Missing Values: Sometimes we do not want to delete data, but rather retain as much information as possible by filling in the missing values. The `fillna()` method can be used to fill missing values, with common strategies including filling with a fixed value, mean, median, or neighboring values.

```python
# Fill all missing values with 0
df_filled_zero = df.fillna(0)
print(df_filled_zero)

# Fill missing values with the previous valid value (forward fill)
df_filled_ffill = df.fillna(method='ffill')
print(df_filled_ffill)

# Fill missing values with the next valid value (backward fill)
df_filled_bfill = df.fillna(method='bfill')
print(df_filled_bfill)

# Fill missing values in each column with the column's mean (example with 'Age' column)
df['Age'] = df['Age'].fillna(df['Age'].mean())
print(df)
```

Run the above code separately, and the results are as follows:

```shell
# Original DataFrame
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob   NaN  60000.0
2   None  30.0      NaN

# Fill all missing values with 0:
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob   0.0  60000.0
2      0  30.0      0.0

# Fill missing values in 'Age' column with the column's mean:
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob  27.5  60000.0
2   None  30.0      NaN

# Fill missing values with the previous valid value:
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob  25.0  60000.0
2    Bob  30.0  60000.0

# Fill missing values with the next valid value:
    Name   Age   Salary
0  Alice  25.0  50000.0
1    Bob  30.0  60000.0
2   None  30.0      NaN
```

Pandas provides powerful functions for handling missing values, including finding, counting, deleting, and filling. In data cleaning, the choice of appropriate missing value handling methods depends on the specific situation of the data. By using `dropna()` and `fillna()` appropriately, you can effectively maintain data integrity and the accuracy of analysis results.

### 2. Handling Duplicate Values

Duplicate data can lead to biased analysis results, so it is necessary to remove duplicates.

![](http://szms-python-images.oss-cn-hangzhou.aliyuncs.com/Pandas/%E6%96%B0%E7%89%88%E8%84%91%E5%9B%BE/%E9%87%8D%E5%A4%8D%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86en.png)

1. Finding Duplicate Data: You can use the `duplicated()` method to find duplicate data. This method returns a boolean sequence indicating whether each row is a duplicate.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create a sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Alice', 'David', 'Bob'],
    'Age': [25, 30, 25, 35, 30],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles']
}
df = pd.DataFrame(data)

# Find duplicate rows
print(df.duplicated())
```

::

2. Removing Duplicate Data: You can use the `drop_duplicates()` method to remove duplicate rows. By using the `subset` parameter, you can specify the columns to check for duplicates. If not specified, all columns are checked.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Remove duplicate rows
df_unique = df.drop_duplicates()
print(df_unique)

# Remove duplicate rows based only on the 'Name' column
df_unique_name = df.drop_duplicates(subset=['Name'])
print(df_unique_name)
```

::

### 3. Converting Data Types

Sometimes, data may have incorrect data types, such as an age field stored as a string. You can use the `astype()` method to convert columns to the correct data type.

```python
import pandas as pd

# Create a sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': ['25', '30', '35', '40'],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)

# Convert the 'Age' column to integer type
df['Age'] = df['Age'].astype(int)

# View the data types of the DataFrame
print(df.dtypes)
```

Output:

```shell
Name    object
Age      int64
City    object
dtype: object
```

Sometimes, the data type may be correct, but the data format is incorrect. For example, a date column may contain dates in multiple formats. We can use `to_datetime` and `strftime` to unify the format:

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create a DataFrame with multiple date formats
data = {'Date': ['2023-01-01 12:34:56', '2023/02/02', '03/04/2023', '2023-05-06T10:11:12Z']}
df = pd.DataFrame(data)

# Use the to_datetime() function, specifying error handling as coerce, and try multiple common date formats
df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)

# Unify the date format to "YYYY-MM-DD"
df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')

print(df)
```

::

### 5. Data Standardization

To ensure data standardization, you can convert string fields to lowercase or uppercase, or remove extra spaces.

```python
import pandas as pd

# Create a sample DataFrame
data = {
    'Name': [' Alice ', 'Bob', ' CHARLIE ', 'David '],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)

# Convert the 'Name' column to lowercase
df['Name'] = df['Name'].str.lower()

# Remove extra spaces in the 'Name' column
df['Name'] = df['Name'].str.strip()

# Output the processed DataFrame
print(df)
```

```shell
Name  Age         City
0    alice   25     New York
1      bob   30  Los Angeles
2  charlie   35      Chicago
3    david   40      Houston
```

### 6. Case Study

Follow the case in the link below to review the content of this chapter:

[https://www.kaggle.com/code/zhaowenxi/pandas-for-python-2](https://www.kaggle.com/code/zhaowenxi/pandas-for-python-2)

Assume there is a DataFrame `df` containing employee information, with the initial state as follows:

| Name   | Age | Salary | City       |
|--------|-----|--------|------------|
| Alice  | 25  | 50000  | New York   |
| Bob    | NaN | 60000  | Los Angeles|
| NULL   | 30  | NaN    | Chicago    |
| Alice  | 25  | 50000  | New York   |
| David  | 35  | 65000  | Houston    |

You need to perform the following data cleaning tasks:

1. **Find and count missing values**:
   - Use the `isna()` method to find missing values in the DataFrame and print the results.
   - Use the `sum()` method to count the number of missing values in each column and print the results.

2. **Handle missing values**:
   - Delete rows that contain any missing values and print the results.
   - Delete rows that have missing values only in the `Name` and `Age` columns and print the results.
   - Keep rows that have at least two non-missing values and print the results.
   - Fill missing values in each column with the mean value (using the `Age` column as an example) and print the results.

3. **Handle duplicate values**:
   - Find and print all duplicate rows.
   - Delete all duplicate rows and print the results.
   - Delete duplicate rows based only on the `Name` column and print the results.

4. **Convert data types**:
   - Convert the data type of the `Age` column from string to integer and print the results after the conversion.

5. **Standardize date format**:
   - Assume the `City` column contains data in various date formats. Use the `to_datetime` function to standardize the dates to ISO format (YYYY-MM-DD) and print the results.

6. **Data normalization**:
   - Convert all strings in the `Name` column to lowercase and remove any extra spaces, then print the cleaned DataFrame.

## 5. Summary of Common Pandas Functions

### 1. Reading, Viewing, and Cleaning Data

In the above article, we learned the most basic functions of Pandas:

| Function                                | Description                          |
| --------------------------------------- | ------------------------------------ |
| pd.read_csv(filename)                   | Read a CSV file;                     |
| pd.read_excel(filename)                 | Read an Excel file;                  |
| pd.read_sql(query, connection_object)   | Read data from an SQL database;      |
| pd.read_json(json_string)               | Read data from a JSON string;        |
| pd.read_html(url)                       | Read data from an HTML page;         |
| df.head(n)                              | Display the first n rows of data;    |
| df.tail(n)                              | Display the last n rows of data;     |
| df.info()                               | Display information about the data, including column names, data types, and missing values; |
| df.describe()                           | Display basic statistical information about the data, including mean, variance, maximum, minimum, etc.; |
| df.shape                                | Display the number of rows and columns in the data; |
| df.dropna()                             | Remove rows or columns with missing values; |
| df.fillna(value)                        | Replace missing values with the specified value; |
| df.replace(old_value, new_value)        | Replace specified values with new values; |
| df.duplicated()                         | Check for duplicate data;            |
| df.drop_duplicates()                    | Remove duplicate data.               |

### 2. Data Selection and Slicing

1. Directly access a column by its name: `df[column_name]`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create a DataFrame
data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 28]}
df = pd.DataFrame(data)

# Select the 'Age' column
age_column = df['Age']
print(age_column)
```

::

2. Select data by label: `df.loc[row_index, column_name]`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Select the first row, 'Name' column
first_name = df.loc[0, 'Name']
print(first_name)
```

::

3. Select data by index: `df.iloc[row_index, column_index]`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Select the second row, first column (index starts from 0)
second_row_first_column = df.iloc[1, 0]
print(second_row_first_column)
```

::

4. Select multiple columns by a list of column names: `df.filter(items=[column_name1, column_name2])`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Select the 'Name' and 'Age' columns
selected_columns = df.filter(items=['Name', 'Age'])
print(selected_columns)
```

::

5. Select columns by matching a regular expression: `df.filter(regex='regex')`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Select all columns containing 'Age'
age_related_columns = df.filter(regex='Age')
print(age_related_columns)
```

::

6. Randomly select n rows of data: `df.sample(n)`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Randomly select 2 rows of data
sample_df = df.sample(2)
print(sample_df)
```

::

### 3. Data Sorting

1. Sort a `DataFrame` by the values of specified columns:

```python
DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')
```

Parameter explanation:

- `by`: The column name or list of column names to sort by.
- `axis`: The axis to sort along, 0 for rows, 1 for columns.
- `ascending`: Whether to sort in ascending order, default is True.
- `inplace`: Whether to modify the original DataFrame, default is False.
- `kind`: The sorting algorithm, commonly used are 'quicksort', 'mergesort', 'heapsort'.
- `na_position`: Specify the position of missing values, 'first' to place them at the beginning, 'last' to place them at the end.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create a DataFrame
data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 28]}
df = pd.DataFrame(data)

# Sort by the 'Age' column in ascending order
df_sorted_by_age = df.sort_values('Age')
print(f"Sorted by 'Age' column in ascending order:\n{df_sorted_by_age}")

# Sort by the 'Name' column in descending order
df_sorted_by_name_desc = df.sort_values('Name', ascending=False)
print(f"Sorted by 'Name' column in descending order:\n{df_sorted_by_name_desc}")

# Sort by multiple columns, 'Age' in ascending order, 'Name' in descending order
df_sorted_multi = df.sort_values(['Age', 'Name'], ascending=[True, False])
print(f"Sorted by multiple columns, 'Age' in ascending order, 'Name' in descending order:\n{df_sorted_multi}")
```

::

2. Sort by index

```python
DataFrame.sort_index(axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')
```

Parameter explanation:

- `axis`: The axis to sort along, 0 for rows, 1 for columns.
- `ascending`: Whether to sort in ascending order, default is True.
- `inplace`: Whether to modify the original DataFrame, default is False.
- `kind`: The sorting algorithm, commonly used are 'quicksort', 'mergesort', 'heapsort'.
- `na_position`: Specify the position of missing values, 'first' to place them at the beginning, 'last' to place them at the end.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Create a DataFrame with string index
data = {'A': [1, 2, 3], 'B': [4, 5, 6]}
df = pd.DataFrame(data, index=['c', 'a', 'b'])

# Sort by index in ascending order
df_sorted_by_index = df.sort_index()
print(df_sorted_by_index)
```

::

### 4. Data Grouping and Aggregation

1. Group data by a specified column: `df.groupby(column_name)`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
import pandas as pd

# Create a DataFrame
data = {'Name': ['Alice', 'Bob', 'Charlie', 'Alice'], 'Age': [25, 30, 28, 25], 'City': ['New York', 'Los Angeles', 'Chicago', 'New York']}
df = pd.DataFrame(data)

# Group by the 'City' column
grouped = df.groupby('City')

# Calculate the average age for each group
print(grouped['Age'].mean())
```

::

2. Apply aggregation functions to grouped data: `df.aggregate({column_name:[function_name]})`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Calculate the mean and max for the grouped data
print(grouped.aggregate({'Age': ['mean', 'max']}))
```

::

3. Create a pivot table

```python
DataFrame.pivot_table(values, index, columns, aggfunc)
```

Parameter explanation:

- `values`: The column to aggregate.
- `index`: The column(s) to use as row index.
- `columns`: The column(s) to use as column index.
- `aggfunc`: The aggregation function.

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Create a pivot table, grouping by city and age, and counting the number of people in each group
pivot_table = df.pivot_table(values='Name', index='City', columns='Age', aggfunc='count')
print(pivot_table)
```

::

### 5. Data Merging

1. Concatenate multiple DataFrames along rows or columns: `pd.concat([df1, df2])`

```python
# Create two DataFrames
df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})

# Concatenate along rows
df_concat_rows = pd.concat([df1, df2])
print(df_concat_rows)

# Concatenate along columns
df_concat_cols = pd.concat([df1, df2], axis=1)
print(df_concat_cols)
```

Output:

```shell
# Concatenate along rows:
   A  B
0  1  3
1  2  4
0  5  7
1  6  8
# Concatenate along columns:
   A  B  A  B
0  1  3  5  7
1  2  4  6  8
```

2. Merge two DataFrames based on a specified column, similar to SQL JOIN: `pd.merge(df1, df2, on=column_name)`

```python
# Create two DataFrames with a common column 'Name'
df1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]})
df2 = pd.DataFrame({'Name': ['Alice', 'Charlie'], 'City': ['New York', 'Chicago']})

# Merge on the 'Name' column
df_merged = pd.merge(df1, df2, on='Name')
print(df_merged)
```

Output:

```shell
Name  Age      City
0  Alice   25  New York
```

### 6. Data Selection and Filtering

1. Conditional filtering: `df[df['column_name'] > value]`

```python
import pandas as pd

# Create a sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'City': ['New York', 'Los Angeles', 'New York', 'Chicago']
}
df = pd.DataFrame(data)

# Conditional filtering: select people older than 25
df_filtered = df[df['Age'] > 25]
print(df_filtered)
```

Output:

```shell
Name  Age         City
1      Bob   30  Los Angeles
2  Charlie   35     New York
3    David   40      Chicago
```

2. Select rows that meet conditions using a string expression: `df.query('column_name > value')`

::code-mirror-run{:editable="true" maxHeight="20rem" :showInCol="true"}

```python
# Select rows that meet conditions using a string expression: select people older than 25 and from New York
df_filtered_query = df.query('Age > 25 and City == "New York"')
print(df_filtered_query)
```

::

### 7. Case Study

Use the following case exercises to become familiar with the important functions mentioned in this chapter:

[https://www.kaggle.com/code/zhaowenxi/pandas-for-python-3](https://www.kaggle.com/code/zhaowenxi/pandas-for-python-3)

As a data analyst, you have an employee dataset containing information about employees' names, ages, cities, and departments. Your task is to perform a series of data processing operations using the Pandas library. Here is the initial state of the dataset:

| Name    | Age | City        | Department |
|---------|-----|-------------|------------|
| Alice   | 25  | New York    | HR         |
| Bob     | 30  | Los Angeles | Sales      |
| Charlie | 28  | Chicago     | IT         |
| David   | 40  | Chicago     | IT         |
| Alice   | 25  | New York    | HR         |

Please complete the following data processing tasks:

1. **Data Reading and Viewing**:
   - Read the above dataset and create a DataFrame `df`.
   - Use the `head()` method to view the first two rows of the dataset.

2. **Data Cleaning**:
   - Use the `dropna()` method to delete rows containing missing values.
   - Use the `fillna()` method to replace all missing age values with the mean of the age column.

3. **Data Selection and Slicing**:
   - Directly obtain the "Age" column data by column name.
   - Use `loc` to select the "Name" column data of the second row.

4. **Data Sorting**:
   - Sort the dataset in ascending order by the "Age" column and print the result.

5. **Data Grouping and Aggregation**:
   - Group the data by the "City" column and calculate the average age of employees in each city.

6. **Data Merging**:
   - Assume there is another DataFrame `df_departments` containing department budget information as follows:
     ```
     Department  Budget
     HR          10000
     Sales       20000
     IT          15000
     ```
   - Use the `merge()` method to merge `df` and `df_departments` by the "Department" column and print the result.
